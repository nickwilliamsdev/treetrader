{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "693727e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nick5\\miniconda3\\envs\\tree-env\\Lib\\site-packages\\requests\\__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from networks.attention_agent import CausalAttentionAgent\n",
    "import torch\n",
    "from utils.trading_gym_env import TradingEnv\n",
    "from utils.synthetic_data_service import SyntheticOHLCVGenerator\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from diffevo import DDIMScheduler, BayesianGenerator\n",
    "from torch.nn.utils import parameters_to_vector, vector_to_parameters\n",
    "from utils.fitess_funcs import batched_fitness_function\n",
    "from api_wrappers.kraken_wrapper import KrakenWrapper\n",
    "kw = KrakenWrapper()\n",
    "dfs = kw.load_hist_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f9e2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      open      high       low     close      vwap           vol  \\\n",
      "0  1707091200  0.493367  0.505404  0.488000  0.494246  0.494287  1.039293e+06   \n",
      "1  1707177600  0.494246  0.504000  0.489976  0.498563  0.498556  6.057023e+05   \n",
      "2  1707264000  0.499144  0.502357  0.466064  0.502000  0.477648  5.491872e+06   \n",
      "3  1707350400  0.502137  0.535569  0.502135  0.529962  0.517441  7.744015e+06   \n",
      "4  1707436800  0.530756  0.548144  0.528513  0.540046  0.538431  2.068370e+06   \n",
      "\n",
      "   log_return     ma_10     ma_50     fib_0   fib_236   fib_382   fib_500  \\\n",
      "0    0.000520  0.503416  0.545709  0.548091  0.524470  0.509856  0.498046   \n",
      "1    0.008697  0.504675  0.543625  0.539343  0.517786  0.504450  0.493672   \n",
      "2    0.006870  0.505907  0.542157  0.537347  0.516261  0.503216  0.492673   \n",
      "3    0.054205  0.506391  0.540956  0.537347  0.516261  0.503216  0.492673   \n",
      "4    0.018849  0.509096  0.539047  0.537347  0.516261  0.503216  0.492673   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.486235     0.448  \n",
      "1  0.482893     0.448  \n",
      "2  0.482131     0.448  \n",
      "3  0.482131     0.448  \n",
      "4  0.482131     0.448  \n",
      "         date     open     high      low    close     vwap            vol  \\\n",
      "0  1707091200  0.15869  0.17213  0.15663  0.15996  0.16116  139660.843310   \n",
      "1  1707177600  0.15916  0.16250  0.15820  0.16237  0.16020   55494.900907   \n",
      "2  1707264000  0.16300  0.16850  0.16047  0.16802  0.16570  212080.108215   \n",
      "3  1707350400  0.16909  0.17056  0.16636  0.16968  0.16906  112024.963466   \n",
      "4  1707436800  0.16928  0.17542  0.16924  0.17501  0.17184  265855.200745   \n",
      "\n",
      "   log_return     ma_10     ma_50    fib_0   fib_236   fib_382   fib_500  \\\n",
      "0    0.005957  0.162906  0.192375  0.19869  0.186866  0.179552  0.173640   \n",
      "1    0.014954  0.162394  0.191713  0.19869  0.186866  0.179552  0.173640   \n",
      "2    0.034205  0.162728  0.191225  0.19330  0.182748  0.176221  0.170945   \n",
      "3    0.009831  0.162797  0.190656  0.17922  0.171991  0.167519  0.163905   \n",
      "4    0.030929  0.163866  0.189684  0.17451  0.168393  0.164609  0.161550   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.167728   0.14859  \n",
      "1  0.167728   0.14859  \n",
      "2  0.165669   0.14859  \n",
      "3  0.160291   0.14859  \n",
      "4  0.158491   0.14859  \n",
      "         date    open    high     low   close    vwap            vol  \\\n",
      "0  1707091200  1.3440  1.3849  1.3346  1.3629  1.3725    1766.689726   \n",
      "1  1707177600  1.3768  1.3871  1.3546  1.3546  1.3679    2584.097509   \n",
      "2  1707264000  1.3534  1.3963  1.3440  1.3963  1.3622    7240.784391   \n",
      "3  1707350400  1.3941  1.4057  1.3819  1.4017  1.3902  104373.227528   \n",
      "4  1707436800  1.4057  1.4465  1.3998  1.4280  1.4168    6809.287977   \n",
      "\n",
      "   log_return    ma_10     ma_50   fib_0   fib_236   fib_382  fib_500  \\\n",
      "0    0.000954  1.40935  1.492862  1.6833  1.577784  1.512508  1.45975   \n",
      "1   -0.006109  1.40370  1.486992  1.6833  1.577784  1.512508  1.45975   \n",
      "2    0.030320  1.40463  1.483372  1.5399  1.468227  1.423887  1.38805   \n",
      "3    0.003860  1.40182  1.479032  1.5348  1.464330  1.420735  1.38550   \n",
      "4    0.018589  1.40202  1.474146  1.5348  1.464330  1.420735  1.38550   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  1.406992    1.2362  \n",
      "1  1.406992    1.2362  \n",
      "2  1.352213    1.2362  \n",
      "3  1.350265    1.2362  \n",
      "4  1.350265    1.2362  \n",
      "         date    open     high     low    close    vwap           vol  \\\n",
      "0  1707091200  8.9661   9.1732  8.8533   9.0394  9.0176   7855.075322   \n",
      "1  1707177600  9.0642   9.2255  9.0640   9.1452  9.1429   7614.705352   \n",
      "2  1707264000  9.1469   9.6429  9.1469   9.5828  9.4261  12738.023438   \n",
      "3  1707350400  9.6470  10.2000  9.4500   9.6399  9.6476  21355.328892   \n",
      "4  1707436800  9.6472  10.1877  9.6472  10.0337  9.9863  11240.276390   \n",
      "\n",
      "   log_return    ma_10      ma_50    fib_0    fib_236   fib_382  fib_500  \\\n",
      "0    0.008153  9.25410  10.163354  10.5629  10.127362  9.857919  9.64015   \n",
      "1    0.011636  9.19871  10.128948  10.3430   9.959358  9.722021  9.53020   \n",
      "2    0.046741  9.21599  10.112584  10.2564   9.893196  9.668502  9.48690   \n",
      "3    0.005941  9.21559  10.091810  10.0605   9.743528  9.547436  9.38895   \n",
      "4    0.040039  9.28466  10.064248  10.2000   9.850106  9.633647  9.45870   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  9.422381    8.7174  \n",
      "1  9.338379    8.7174  \n",
      "2  9.305298    8.7174  \n",
      "3  9.230464    8.7174  \n",
      "4  9.283753    8.7174  \n",
      "         date   open   high    low  close   vwap           vol  log_return  \\\n",
      "0  1707091200  34.80  35.86  34.14  34.57  34.99   4230.706088   -0.009214   \n",
      "1  1707177600  34.42  34.70  33.77  34.14  34.18   3315.612542   -0.012517   \n",
      "2  1707264000  34.09  35.50  33.69  35.38  34.88   4761.027684    0.035677   \n",
      "3  1707350400  35.36  36.06  35.15  35.43  35.55   7821.564859    0.001412   \n",
      "4  1707436800  35.42  38.60  35.41  37.99  37.22  16187.877595    0.069764   \n",
      "\n",
      "    ma_10    ma_50  fib_0   fib_236   fib_382  fib_500   fib_618  fib_1000  \n",
      "0  34.826  37.2764  37.59  35.15448  33.64776    32.43  31.21224     27.27  \n",
      "1  34.913  37.1374  37.59  35.15448  33.64776    32.43  31.21224     27.27  \n",
      "2  34.967  37.0490  37.59  35.15448  33.64776    32.43  31.21224     27.27  \n",
      "3  34.906  36.8930  37.59  35.15448  33.64776    32.43  31.21224     27.27  \n",
      "4  35.200  36.7346  37.59  35.15448  33.64776    32.43  31.21224     27.27  \n",
      "         date    open    high     low   close    vwap         vol  log_return  \\\n",
      "0  1707091200  235.87  237.92  234.82  235.61  235.72   24.011668    0.007070   \n",
      "1  1707177600  236.52  237.71  234.93  235.45  236.09   49.406756   -0.000679   \n",
      "2  1707264000  235.53  240.82  234.74  240.32  236.10   42.824302    0.020473   \n",
      "3  1707350400  240.86  245.03  240.58  244.55  243.40   74.170949    0.017448   \n",
      "4  1707436800  244.60  253.64  244.59  250.81  249.60  140.166636    0.025276   \n",
      "\n",
      "     ma_10     ma_50   fib_0    fib_236    fib_382  fib_500    fib_618  \\\n",
      "0  237.635  242.7518  255.98  247.27632  241.89184  237.540  233.18816   \n",
      "1  236.777  242.9018  250.93  243.41812  238.77094  235.015  231.25906   \n",
      "2  237.173  243.2076  245.03  238.91052  235.12474  232.065  229.00526   \n",
      "3  237.496  243.5126  245.03  238.91052  235.12474  232.065  229.00526   \n",
      "4  238.908  243.8664  245.03  238.91052  235.12474  232.065  229.00526   \n",
      "\n",
      "   fib_1000  \n",
      "0     219.1  \n",
      "1     219.1  \n",
      "2     219.1  \n",
      "3     219.1  \n",
      "4     219.1  \n",
      "         date     open     high      low    close     vwap            vol  \\\n",
      "0  1707091200  0.99948  1.00141  0.99920  1.00128  1.00041  166986.583437   \n",
      "1  1707177600  1.00123  1.00147  0.99930  1.00015  1.00098  184640.783923   \n",
      "2  1707264000  1.00027  1.00090  0.99840  0.99956  0.99997  229145.743545   \n",
      "3  1707350400  0.99891  1.00034  0.99860  0.99970  0.99993  725096.791826   \n",
      "4  1707436800  0.99979  1.00025  0.99897  0.99950  0.99982  607412.172808   \n",
      "\n",
      "   log_return     ma_10     ma_50    fib_0   fib_236   fib_382   fib_500  \\\n",
      "0    0.001039  0.999874  0.999713  1.00210  0.999893  0.998528  0.997425   \n",
      "1   -0.001129  0.999946  0.999716  1.00210  0.999893  0.998528  0.997425   \n",
      "2   -0.000590  1.000030  0.999708  1.00210  0.999893  0.998528  0.997425   \n",
      "3    0.000140  1.000009  0.999737  1.00150  0.999435  0.998158  0.997125   \n",
      "4   -0.000200  0.999964  0.999734  1.00148  0.999420  0.998145  0.997115   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.996322   0.99275  \n",
      "1  0.996322   0.99275  \n",
      "2  0.996322   0.99275  \n",
      "3  0.996093   0.99275  \n",
      "4  0.996085   0.99275  \n",
      "         date    open    high     low   close    vwap            vol  \\\n",
      "0  1707091200  6.6731  6.9742  6.6000  6.7341  6.8192   37073.076617   \n",
      "1  1707177600  6.6980  6.8269  6.6489  6.8062  6.7526   27313.810741   \n",
      "2  1707264000  6.7993  6.9588  6.6716  6.9389  6.8016   26199.028742   \n",
      "3  1707350400  6.9472  7.1014  6.8728  7.0127  6.9923  109157.463652   \n",
      "4  1707436800  7.0185  7.3320  6.9669  7.1843  7.1380   77326.943713   \n",
      "\n",
      "   log_return    ma_10     ma_50   fib_0   fib_236   fib_382  fib_500  \\\n",
      "0    0.008755  6.80287  7.427778  7.6790  7.277163  7.028569  6.82765   \n",
      "1    0.010650  6.81661  7.426406  7.5892  7.208556  6.973072  6.78275   \n",
      "2    0.019309  6.82414  7.430892  7.3836  7.051477  6.846011  6.67995   \n",
      "3    0.010580  6.82233  7.432344  7.2970  6.985315  6.792493  6.63665   \n",
      "4    0.024175  6.85790  7.407570  7.2970  6.985315  6.792493  6.63665   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  6.626731    5.9763  \n",
      "1  6.592428    5.9763  \n",
      "2  6.513889    5.9763  \n",
      "3  6.480807    5.9763  \n",
      "4  6.480807    5.9763  \n",
      "         date    open    high     low   close    vwap            vol  \\\n",
      "0  1702080000  0.7999  0.9205  0.7999  0.8564  0.8571  127444.728200   \n",
      "1  1702166400  0.8515  0.9200  0.7501  0.8337  0.8374  111636.759417   \n",
      "2  1702252800  0.8379  0.8399  0.7271  0.7681  0.7703  163043.665920   \n",
      "3  1702339200  0.7690  0.7923  0.7576  0.7814  0.7777   41052.404104   \n",
      "4  1702425600  0.7806  0.7896  0.7425  0.7849  0.7670   63201.310006   \n",
      "\n",
      "   log_return    ma_10     ma_50   fib_0   fib_236   fib_382  fib_500  \\\n",
      "0    0.072008  0.73903  0.681178  0.7985  0.760929  0.737686  0.71890   \n",
      "1   -0.026864  0.75472  0.686680  0.9205  0.854137  0.813082  0.77990   \n",
      "2   -0.081954  0.76260  0.690814  0.9205  0.854137  0.813082  0.77990   \n",
      "3    0.017167  0.77091  0.694608  0.9205  0.855671  0.815565  0.78315   \n",
      "4    0.004469  0.77928  0.698452  0.9205  0.858243  0.819728  0.78860   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.700114    0.6393  \n",
      "1  0.746718    0.6393  \n",
      "2  0.746718    0.6393  \n",
      "3  0.750735    0.6458  \n",
      "4  0.757472    0.6567  \n",
      "         date     open     high      low    close     vwap          vol  \\\n",
      "0  1707091200  2289.71  2335.46  2273.19  2301.59  2306.25  1070.989308   \n",
      "1  1707177600  2301.69  2390.00  2299.06  2372.72  2342.57  1627.276077   \n",
      "2  1707264000  2372.70  2443.00  2356.00  2424.22  2392.66  4396.383142   \n",
      "3  1707350400  2426.09  2460.86  2412.10  2419.49  2431.08  2282.717514   \n",
      "4  1707436800  2420.85  2522.12  2418.82  2486.07  2472.87  4427.361196   \n",
      "\n",
      "   log_return     ma_10      ma_50    fib_0     fib_236     fib_382   fib_500  \\\n",
      "0    0.005048  2296.985  2339.2874  2611.26  2506.91496  2442.36252  2390.190   \n",
      "1    0.030437  2307.418  2342.3558  2592.60  2492.65872  2430.83064  2380.860   \n",
      "2    0.021473  2324.147  2347.2806  2547.77  2458.40860  2403.12570  2358.445   \n",
      "3   -0.001953  2334.585  2351.6484  2503.42  2424.52520  2375.71740  2336.270   \n",
      "4    0.027146  2348.897  2356.6040  2490.56  2414.70016  2367.76992  2329.840   \n",
      "\n",
      "      fib_618  fib_1000  \n",
      "0  2338.01748   2169.12  \n",
      "1  2330.88936   2169.12  \n",
      "2  2313.76430   2169.12  \n",
      "3  2296.82260   2169.12  \n",
      "4  2291.91008   2169.12  \n",
      "         date      open      high       low     close      vwap           vol  \\\n",
      "0  1707091200  18.19368  19.87899  17.81399  19.12157  19.23238  49136.069712   \n",
      "1  1707177600  19.16153  19.32000  18.23190  18.28367  18.68099  28294.083958   \n",
      "2  1707264000  18.30375  19.05974  18.07749  18.82818  18.48181  28619.063881   \n",
      "3  1707350400  18.82899  19.25333  17.99595  18.21643  18.55123  34034.325531   \n",
      "4  1707436800  18.21535  18.67798  18.02490  18.50287  18.39911  26423.494590   \n",
      "\n",
      "   log_return      ma_10      ma_50     fib_0    fib_236    fib_382   fib_500  \\\n",
      "0    0.048991  16.463410  15.175801  18.88923  17.622637  16.839067  16.20577   \n",
      "1   -0.044809  16.857954  15.248095  19.87899  18.378814  17.450738  16.70065   \n",
      "2    0.029346  17.289872  15.342600  19.87899  18.378814  17.450738  16.70065   \n",
      "3   -0.033031  17.614106  15.422140  19.87899  18.378814  17.450738  16.70065   \n",
      "4    0.015602  17.918615  15.486197  19.87899  18.378814  17.450738  16.70065   \n",
      "\n",
      "     fib_618  fib_1000  \n",
      "0  15.572473  13.52231  \n",
      "1  15.950562  13.52231  \n",
      "2  15.950562  13.52231  \n",
      "3  15.950562  13.52231  \n",
      "4  15.950562  13.52231  \n",
      "         date      open      high       low     close      vwap          vol  \\\n",
      "0  1707091200  66.91000  68.20531  66.88637  67.67111  67.73194  3035.187138   \n",
      "1  1707177600  67.68316  68.73732  67.61775  68.36322  68.13527  4059.878922   \n",
      "2  1707264000  68.30852  68.75698  67.76985  68.54862  68.13864  4594.376867   \n",
      "3  1707350400  68.56153  70.58283  68.44632  70.58283  69.03967  3568.861342   \n",
      "4  1707436800  70.55726  71.83534  70.17994  70.65001  70.89275  3146.993736   \n",
      "\n",
      "   log_return      ma_10      ma_50     fib_0    fib_236    fib_382   fib_500  \\\n",
      "0    0.010410  67.807166  69.664159  72.83205  70.558889  69.152611  68.01603   \n",
      "1    0.010176  67.831665  69.614354  72.83205  70.558889  69.152611  68.01603   \n",
      "2    0.002708  67.854237  69.575293  72.83205  70.558889  69.152611  68.01603   \n",
      "3    0.029244  68.068910  69.592465  72.83205  70.558889  69.152611  68.01603   \n",
      "4    0.000951  68.394808  69.588009  72.83205  70.558889  69.152611  68.01603   \n",
      "\n",
      "     fib_618  fib_1000  \n",
      "0  66.879449  63.20001  \n",
      "1  66.879449  63.20001  \n",
      "2  66.879449  63.20001  \n",
      "3  66.879449  63.20001  \n",
      "4  66.879449  63.20001  \n",
      "         date     open     high      low    close     vwap           vol  \\\n",
      "0  1707091200  0.43078  0.43943  0.42620  0.43276  0.43626   7748.945952   \n",
      "1  1707177600  0.43366  0.43731  0.42991  0.43398  0.43306  19002.889794   \n",
      "2  1707264000  0.43273  0.44787  0.42475  0.44616  0.43688  21953.397757   \n",
      "3  1707350400  0.44578  0.45354  0.44209  0.44960  0.44809  16695.827771   \n",
      "4  1707436800  0.45448  0.47670  0.45388  0.46778  0.46178  27520.732497   \n",
      "\n",
      "   log_return     ma_10     ma_50    fib_0   fib_236   fib_382   fib_500  \\\n",
      "0    0.002522  0.442096  0.474052  0.49421  0.472359  0.458841  0.447915   \n",
      "1    0.002815  0.439986  0.473264  0.49421  0.472359  0.458841  0.447915   \n",
      "2    0.027679  0.440043  0.472654  0.49421  0.472359  0.458841  0.447915   \n",
      "3    0.007681  0.439089  0.471664  0.49421  0.472359  0.458841  0.447915   \n",
      "4    0.039640  0.441349  0.470603  0.49421  0.472359  0.458841  0.447915   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.436989   0.40162  \n",
      "1  0.436989   0.40162  \n",
      "2  0.436989   0.40162  \n",
      "3  0.436989   0.40162  \n",
      "4  0.436989   0.40162  \n",
      "         date      open      high       low     close      vwap           vol  \\\n",
      "0  1707091200  0.000009  0.000009  0.000009  0.000009  0.000009  1.466800e+09   \n",
      "1  1707177600  0.000009  0.000009  0.000009  0.000009  0.000009  5.283345e+08   \n",
      "2  1707264000  0.000009  0.000009  0.000009  0.000009  0.000009  1.641321e+09   \n",
      "3  1707350400  0.000009  0.000009  0.000009  0.000009  0.000009  1.985413e+09   \n",
      "4  1707436800  0.000009  0.000009  0.000009  0.000009  0.000009  1.241880e+09   \n",
      "\n",
      "   log_return     ma_10    ma_50    fib_0  fib_236   fib_382   fib_500  \\\n",
      "0   -0.004499  0.000009  0.00001  0.00001  0.00001  0.000009  0.000009   \n",
      "1    0.000000  0.000009  0.00001  0.00001  0.00001  0.000009  0.000009   \n",
      "2    0.020090  0.000009  0.00001  0.00001  0.00001  0.000009  0.000009   \n",
      "3    0.015351  0.000009  0.00001  0.00001  0.00001  0.000009  0.000009   \n",
      "4    0.021529  0.000009  0.00001  0.00001  0.00001  0.000009  0.000009   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.000009  0.000008  \n",
      "1  0.000009  0.000008  \n",
      "2  0.000009  0.000008  \n",
      "3  0.000009  0.000008  \n",
      "4  0.000009  0.000008  \n",
      "         date    open    high     low   close    vwap           vol  \\\n",
      "0  1707091200   95.48   98.66   94.28   95.65   96.14  13459.520407   \n",
      "1  1707177600   95.75   97.85   92.97   96.94   95.30  16695.326483   \n",
      "2  1707264000   96.99  101.47   94.71  100.80   98.33  18946.061787   \n",
      "3  1707350400  101.11  104.92  100.60  102.80  102.34  28130.616587   \n",
      "4  1707436800  103.00  109.07  102.98  106.79  105.60  43171.273441   \n",
      "\n",
      "   log_return   ma_10    ma_50   fib_0   fib_236   fib_382  fib_500   fib_618  \\\n",
      "0    0.000105  97.752  97.1226  106.38  99.91832  95.92084    92.69  89.45916   \n",
      "1    0.013397  98.029  97.5730  106.38  99.91832  95.92084    92.69  89.45916   \n",
      "2    0.039046  98.506  98.1286  106.38  99.91832  95.92084    92.69  89.45916   \n",
      "3    0.019647  98.631  98.5426  106.38  99.91832  95.92084    92.69  89.45916   \n",
      "4    0.038079  99.167  98.7990  106.38  99.91832  95.92084    92.69  89.45916   \n",
      "\n",
      "   fib_1000  \n",
      "0      79.0  \n",
      "1      79.0  \n",
      "2      79.0  \n",
      "3      79.0  \n",
      "4      79.0  \n",
      "         date    open    high     low   close    vwap           vol  \\\n",
      "0  1707091200  1.0004  1.0059  1.0002  1.0013  1.0011  1.190193e+07   \n",
      "1  1707177600  1.0012  1.0016  1.0001  1.0003  1.0008  8.503174e+06   \n",
      "2  1707264000  1.0003  1.0010  1.0002  1.0002  1.0006  1.233937e+07   \n",
      "3  1707350400  1.0002  1.0006  0.9996  0.9999  1.0001  1.174601e+07   \n",
      "4  1707436800  0.9998  1.0003  0.9991  0.9995  0.9998  1.069683e+07   \n",
      "\n",
      "   log_return    ma_10     ma_50   fib_0  fib_236   fib_382  fib_500  \\\n",
      "0    0.000799  1.00038  1.000104  1.0079  1.00587  1.004615   1.0036   \n",
      "1   -0.000999  1.00041  1.000106  1.0079  1.00587  1.004615   1.0036   \n",
      "2   -0.000100  1.00041  1.000104  1.0079  1.00587  1.004615   1.0036   \n",
      "3   -0.000300  1.00040  1.000102  1.0079  1.00587  1.004615   1.0036   \n",
      "4   -0.000400  1.00035  1.000094  1.0079  1.00587  1.004615   1.0036   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  1.002585    0.9993  \n",
      "1  1.002585    0.9993  \n",
      "2  1.002585    0.9993  \n",
      "3  1.002585    0.9993  \n",
      "4  1.002585    0.9993  \n",
      "         date     open     high      low    close     vwap           vol  \\\n",
      "0  1707091200  0.02495  0.02610  0.02495  0.02504  0.02519  92934.610834   \n",
      "1  1707177600  0.02538  0.02538  0.02495  0.02522  0.02518  35212.539487   \n",
      "2  1707264000  0.02494  0.02573  0.02494  0.02562  0.02563  61817.904767   \n",
      "3  1707350400  0.02575  0.02610  0.02566  0.02586  0.02579  34747.675050   \n",
      "4  1707436800  0.02566  0.02647  0.02566  0.02647  0.02625  97755.864044   \n",
      "\n",
      "   log_return     ma_10     ma_50    fib_0   fib_236   fib_382   fib_500  \\\n",
      "0   -0.003190  0.025425  0.028845  0.03228  0.030017  0.028617  0.027485   \n",
      "1    0.007163  0.025289  0.028679  0.03228  0.030017  0.028617  0.027485   \n",
      "2    0.015736  0.025256  0.028508  0.03228  0.030017  0.028617  0.027485   \n",
      "3    0.009324  0.025189  0.028359  0.02956  0.027939  0.026936  0.026125   \n",
      "4    0.023315  0.025270  0.028193  0.02821  0.026907  0.026101  0.025450   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.026353   0.02269  \n",
      "1  0.026353   0.02269  \n",
      "2  0.026353   0.02269  \n",
      "3  0.025314   0.02269  \n",
      "4  0.024799   0.02269  \n",
      "         date     open     high      low    close     vwap         vol  \\\n",
      "0  1707091200  42582.5  43521.1  42285.9  42709.4  42865.8  200.193993   \n",
      "1  1707177600  42704.4  43367.9  42575.0  43088.6  43142.4  332.670882   \n",
      "2  1707264000  43099.6  44364.2  42050.1  44346.3  43427.4  341.938019   \n",
      "3  1707350400  44344.6  45584.9  44341.8  45289.3  45009.7  450.988371   \n",
      "4  1707436800  45290.0  48148.7  45253.5  47127.8  46970.3  632.594358   \n",
      "\n",
      "   log_return     ma_10      ma_50    fib_0     fib_236     fib_382   fib_500  \\\n",
      "0    0.002917  42750.21  42935.946  43838.0  42595.5780  41826.9610  41205.75   \n",
      "1    0.008839  42847.07  42944.720  43838.0  42595.5780  41826.9610  41205.75   \n",
      "2    0.028771  43078.52  42986.060  43838.0  42595.5780  41826.9610  41205.75   \n",
      "3    0.021042  43279.61  43019.030  44364.2  42997.5948  42152.1526  41468.85   \n",
      "4    0.039792  43697.33  43084.344  45584.9  43930.2096  42906.5452  42079.20   \n",
      "\n",
      "      fib_618  fib_1000  \n",
      "0  40584.5390   38573.5  \n",
      "1  40584.5390   38573.5  \n",
      "2  40584.5390   38573.5  \n",
      "3  40785.5474   38573.5  \n",
      "4  41251.8548   38573.5  \n",
      "         date     open     high      low    close     vwap           vol  \\\n",
      "0  1707091200  0.07844  0.08022  0.07752  0.07815  0.07893  2.433257e+06   \n",
      "1  1707177600  0.07833  0.07919  0.07781  0.07863  0.07859  7.503678e+05   \n",
      "2  1707264000  0.07861  0.08016  0.07821  0.08003  0.07905  1.271872e+06   \n",
      "3  1707350400  0.08036  0.08075  0.07971  0.07986  0.08003  3.472077e+06   \n",
      "4  1707436800  0.07988  0.08206  0.07988  0.08136  0.08122  1.669222e+06   \n",
      "\n",
      "   log_return     ma_10     ma_50   fib_0   fib_236   fib_382   fib_500  \\\n",
      "0   -0.001662  0.079233  0.084182  0.0918  0.087823  0.085363  0.083375   \n",
      "1    0.006123  0.079071  0.083914  0.0918  0.087823  0.085363  0.083375   \n",
      "2    0.017648  0.079209  0.083710  0.0918  0.087823  0.085363  0.083375   \n",
      "3   -0.002126  0.079068  0.083479  0.0918  0.087823  0.085363  0.083375   \n",
      "4    0.018609  0.079228  0.083206  0.0918  0.088088  0.085791  0.083935   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.081387   0.07495  \n",
      "1  0.081387   0.07495  \n",
      "2  0.081387   0.07495  \n",
      "3  0.081387   0.07495  \n",
      "4  0.082079   0.07607  \n",
      "         date    open    high     low   close    vwap           vol  \\\n",
      "0  1707091200  166.13  167.84  163.92  165.59  165.13   2136.716585   \n",
      "1  1707177600  165.23  166.81   99.88  104.41  115.48  22308.250847   \n",
      "2  1707264000  104.00  134.00  104.00  131.00  125.08   9462.104747   \n",
      "3  1707350400  131.25  134.98  118.21  128.30  124.56   3264.153647   \n",
      "4  1707436800  128.06  128.75  120.04  122.35  122.91   3616.011879   \n",
      "\n",
      "   log_return    ma_10     ma_50   fib_0    fib_236    fib_382  fib_500  \\\n",
      "0   -0.002774  164.358  161.8384  171.93  165.92852  162.21574  159.215   \n",
      "1   -0.461189  158.799  160.4526  171.93  165.92852  162.21574  159.215   \n",
      "2    0.226872  155.828  159.6554  171.93  154.92620  144.40690  135.905   \n",
      "3   -0.020826  151.858  158.7274  171.93  154.92620  144.40690  135.905   \n",
      "4   -0.047485  148.120  157.7084  171.93  154.92620  144.40690  135.905   \n",
      "\n",
      "     fib_618  fib_1000  \n",
      "0  156.21426    146.50  \n",
      "1  156.21426    146.50  \n",
      "2  127.40310     99.88  \n",
      "3  127.40310     99.88  \n",
      "4  127.40310     99.88  \n",
      "         date     open     high      low    close     vwap           vol  \\\n",
      "0  1707091200  0.50337  0.51390  0.49759  0.50673  0.50601  1.578009e+06   \n",
      "1  1707177600  0.50684  0.51068  0.49854  0.50563  0.50448  1.898320e+06   \n",
      "2  1707264000  0.50501  0.51510  0.49985  0.51403  0.50787  1.368228e+06   \n",
      "3  1707350400  0.51379  0.51895  0.51050  0.51365  0.51457  2.098670e+06   \n",
      "4  1707436800  0.51484  0.52778  0.51426  0.52559  0.52227  8.689090e+05   \n",
      "\n",
      "   log_return     ma_10     ma_50    fib_0   fib_236   fib_382   fib_500  \\\n",
      "0    0.006434  0.514871  0.571377  0.57961  0.557289  0.543480  0.532320   \n",
      "1   -0.002173  0.512437  0.569240  0.57625  0.554722  0.541404  0.530640   \n",
      "2    0.016476  0.511444  0.567418  0.56880  0.549030  0.536800  0.526915   \n",
      "3   -0.000740  0.509315  0.565352  0.55500  0.538487  0.528271  0.520015   \n",
      "4    0.022979  0.510817  0.563385  0.55500  0.538487  0.528271  0.520015   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  0.521160   0.48503  \n",
      "1  0.519876   0.48503  \n",
      "2  0.517030   0.48503  \n",
      "3  0.511759   0.48503  \n",
      "4  0.511759   0.48503  \n",
      "         date    open    high     low   close    vwap           vol  \\\n",
      "0  1707091200  0.9586  0.9876  0.9475  0.9598  0.9731   3039.738290   \n",
      "1  1707177600  0.9670  0.9867  0.9670  0.9820  0.9832   7202.728172   \n",
      "2  1707264000  0.9800  1.0078  0.9656  0.9993  0.9867   5669.430827   \n",
      "3  1707350400  1.0094  1.0150  0.9966  1.0138  1.0061   7766.254671   \n",
      "4  1707436800  1.0144  1.0490  0.9779  1.0375  1.0274  45475.018215   \n",
      "\n",
      "   log_return    ma_10     ma_50   fib_0   fib_236   fib_382  fib_500  \\\n",
      "0    0.001355  0.98673  0.997644  1.1929  1.124484  1.082158  1.04795   \n",
      "1    0.022866  0.98475  0.999442  1.1424  1.085902  1.050949  1.02270   \n",
      "2    0.017464  0.98673  1.002140  1.0993  1.052973  1.024313  1.00115   \n",
      "3    0.014406  0.98766  1.004788  1.0881  1.044416  1.017392  0.99555   \n",
      "4    0.023108  0.99163  1.006884  1.0881  1.044416  1.017392  0.99555   \n",
      "\n",
      "    fib_618  fib_1000  \n",
      "0  1.013742     0.903  \n",
      "1  0.994451     0.903  \n",
      "2  0.977987     0.903  \n",
      "3  0.973708     0.903  \n",
      "4  0.973708     0.903  \n",
      "Selected assets for this generation: ['USDCUSDT.txt', 'AVAXUSDT.txt', 'LTCUSDT.txt', 'DOTUSDT.txt', 'ADAUSDT.txt', 'XRPUSDT.txt', 'ATOMUSDT.txt', 'USTUSDT.txt', 'SHIBUSDT.txt', 'ETHUSDT.txt']\n",
      "Batch states shape: torch.Size([3700, 50, 3])\n",
      "Batch price changes shape: torch.Size([3700])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input tensor dimensions do not match the model's sequence length or state dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m DDIMScheduler(num_step\u001b[38;5;241m=\u001b[39mseq_len)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100\u001b[39m):  \u001b[38;5;66;03m# Number of training steps\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m fitness_function(population, agent_model, train_data)\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mmax\u001b[39m(rewards)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    132\u001b[0m     population \u001b[38;5;241m=\u001b[39m scheduler\u001b[38;5;241m.\u001b[39mstep(population, rewards)\n",
      "Cell \u001b[1;32mIn[9], line 122\u001b[0m, in \u001b[0;36mfitness_function\u001b[1;34m(population, agent, train_data, num_assets)\u001b[0m\n\u001b[0;32m    119\u001b[0m     vector_to_parameters(params, agent\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# Evaluate the agent on the batch\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     reward \u001b[38;5;241m=\u001b[39m batched_fitness_function(agent, batch_states, batch_price_changes)\n\u001b[0;32m    123\u001b[0m     rewards\u001b[38;5;241m.\u001b[39mappend(reward\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem())  \u001b[38;5;66;03m# Sum rewards across all assets in the batch\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rewards\n",
      "File \u001b[1;32mc:\\Users\\nick5\\dev\\hypercube_ai\\ai\\treetrader\\utils\\fitess_funcs.py:23\u001b[0m, in \u001b[0;36mbatched_fitness_function\u001b[1;34m(agent, states, price_changes)\u001b[0m\n\u001b[0;32m     20\u001b[0m current_states \u001b[38;5;241m=\u001b[39m states[:, t, :]  \u001b[38;5;66;03m# Shape: (batch_dim, state_dim)\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get actions for all assets in the batch\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m actions \u001b[38;5;241m=\u001b[39m agent(current_states)  \u001b[38;5;66;03m# Shape: (batch_dim, 2) -> 2 actions: Buy, Sell\u001b[39;00m\n\u001b[0;32m     24\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(actions, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Convert to discrete actions (0=Sell, 1=Buy)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Convert actions to -1 (Sell) and 1 (Buy)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nick5\\miniconda3\\envs\\tree-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nick5\\miniconda3\\envs\\tree-env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\nick5\\dev\\hypercube_ai\\ai\\treetrader\\networks\\attention_agent.py:117\u001b[0m, in \u001b[0;36mCausalAttentionAgent.forward\u001b[1;34m(self, state_history)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;124;03mForward pass for the CausalAttentionAgent.\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;124;03m    torch.Tensor: Logits for the actions, of shape (batch_size, action_dim).\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# Ensure the input shape is correct\u001b[39;00m\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m state_history\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len \u001b[38;5;129;01mand\u001b[39;00m state_history\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim, \\\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput tensor dimensions do not match the model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms sequence length or state dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;66;03m# Project the input features\u001b[39;00m\n\u001b[0;32m    121\u001b[0m projected_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(state_history)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Input tensor dimensions do not match the model's sequence length or state dimension."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def add_features(df):\n",
    "    df.columns = [col.lower() for col in df.columns]\n",
    "    print(df.head())\n",
    "    # Ensure columns are numeric\n",
    "    df['close'] = pd.to_numeric(df['close'], errors='coerce')\n",
    "    df['high'] = pd.to_numeric(df['high'], errors='coerce')\n",
    "    df['low'] = pd.to_numeric(df['low'], errors='coerce')\n",
    "    df['vol'] = pd.to_numeric(df['vol'], errors='coerce')\n",
    "\n",
    "    # Add log returns\n",
    "    df['log_return'] = np.log(df['close']).diff()\n",
    "\n",
    "    # Add moving averages\n",
    "    df['ma_10'] = df['close'].rolling(window=10).mean()\n",
    "    df['ma_50'] = df['close'].rolling(window=50).mean()\n",
    "\n",
    "    # Add Fibonacci levels\n",
    "    fib_ratios = [0.0, 0.236, 0.382, 0.5, 0.618, 1.0]\n",
    "    for ratio in fib_ratios:\n",
    "        df[f'fib_{int(ratio * 1000)}'] = np.nan\n",
    "    for i in range(20, len(df)):\n",
    "        high = df['high'].iloc[i-20:i].max()\n",
    "        low = df['low'].iloc[i-20:i].min()\n",
    "        for ratio in fib_ratios:\n",
    "            level = high - (high - low) * ratio\n",
    "            df.at[i, f'fib_{int(ratio * 1000)}'] = level\n",
    "\n",
    "    # Drop NaN rows\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# Add features to all dataframes\n",
    "dfs = {asset: add_features(df) for asset, df in dfs.items()}\n",
    "def split_train_test(df, train_ratio=0.8):\n",
    "    train_size = int(len(df) * train_ratio)\n",
    "    train_df = df.iloc[:train_size]\n",
    "    test_df = df.iloc[train_size:]\n",
    "    return train_df, test_df\n",
    "\n",
    "# Split all dataframes into train/test sets\n",
    "train_test_data = {asset: split_train_test(df) for asset, df in dfs.items()}\n",
    "def prepare_data(df, seq_len):\n",
    "    states = []\n",
    "    price_changes = []\n",
    "\n",
    "    for i in range(len(df) - seq_len):\n",
    "        state = df.iloc[i:i+seq_len][['log_return', 'ma_10', 'ma_50']].values\n",
    "        price_change = df.iloc[i+seq_len]['log_return']\n",
    "        states.append(state)\n",
    "        price_changes.append(price_change)\n",
    "\n",
    "    states = torch.tensor(states, dtype=torch.float32)  # Shape: (num_samples, seq_len, num_features)\n",
    "    price_changes = torch.tensor(price_changes, dtype=torch.float32)  # Shape: (num_samples,)\n",
    "    return states, price_changes\n",
    "\n",
    "# Prepare data for all assets\n",
    "seq_len = 50\n",
    "train_data = {asset: prepare_data(train, seq_len) for asset, (train, _) in train_test_data.items()}\n",
    "test_data = {asset: prepare_data(test, seq_len) for asset, (_, test) in train_test_data.items()}\n",
    "\n",
    "def run(x_array, population, agent):\n",
    "    rewards = []\n",
    "    # Example of a random walk in the environment\n",
    "    for xp in population:\n",
    "        vector_to_parameters(torch.tensor(xp, dtype=torch.float32), agent.parameters())\n",
    "        rewards.append(batched_fitness_function(agent, x_array))\n",
    "    return rewards\n",
    "\n",
    "# Define hyperparameters\n",
    "POP_SIZE = 100\n",
    "SCALING = 0.1\n",
    "\n",
    "# Instantiate the model\n",
    "STATE_DIM = train_data[list(train_data.keys())[0]][0].shape[-1]  # Number of features\n",
    "ACTION_DIM = 2  # Buy, Sell\n",
    "agent_model = CausalAttentionAgent(state_dim=STATE_DIM, action_dim=ACTION_DIM, seq_len=seq_len)\n",
    "\n",
    "# Initialize population\n",
    "dim = parameters_to_vector(agent_model.parameters()).shape[0]\n",
    "population = torch.randn(POP_SIZE, dim) * SCALING\n",
    "\n",
    "# Fitness function\n",
    "def fitness_function(population, agent, train_data, num_assets=10):\n",
    "    \"\"\"\n",
    "    Evaluate the fitness of the population on a randomly selected batch of assets.\n",
    "\n",
    "    Args:\n",
    "        population: The population of agent parameters.\n",
    "        agent: The trading agent model.\n",
    "        train_data: Dictionary of training data for all assets.\n",
    "        num_assets: Number of assets to randomly select for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        List of rewards for each member of the population.\n",
    "    \"\"\"\n",
    "    # Randomly select a batch of assets\n",
    "    selected_assets = random.sample(list(train_data.keys()), num_assets)\n",
    "    print(train_data)\n",
    "    print(f\"Selected assets for this generation: {selected_assets}\")\n",
    "\n",
    "    # Combine states and price changes for the selected assets\n",
    "    batch_states = torch.cat([train_data[asset][0] for asset in selected_assets], dim=0)  # Combine states\n",
    "    batch_price_changes = torch.cat([train_data[asset][1] for asset in selected_assets], dim=0)  # Combine price changes\n",
    "\n",
    "    # Debug: Check shapes\n",
    "    print(\"Batch states shape:\", batch_states.shape)\n",
    "    print(\"Batch price changes shape:\", batch_price_changes.shape)\n",
    "\n",
    "    # Ensure batch_states matches the model's expected input shape\n",
    "    seq_len = agent.seq_len  # Sequence length expected by the model\n",
    "    state_dim = agent.state_dim  # State dimension expected by the model\n",
    "    batch_states = batch_states.view(-1, seq_len, state_dim)\n",
    "\n",
    "    rewards = []\n",
    "    for params in population:\n",
    "        # Update the agent's parameters\n",
    "        vector_to_parameters(params, agent.parameters())\n",
    "\n",
    "        # Evaluate the agent on the batch\n",
    "        reward = batched_fitness_function(agent, batch_states, batch_price_changes)\n",
    "        rewards.append(reward.sum().item())  # Sum rewards across all assets in the batch\n",
    "\n",
    "    return rewards\n",
    "\n",
    "# Train with diffusion evolution\n",
    "scheduler = DDIMScheduler(num_step=seq_len)\n",
    "for step in range(100):  # Number of training steps\n",
    "    rewards = fitness_function(population, agent_model, train_data)\n",
    "    print(f\"Step {step}, Best Reward: {max(rewards)}\")\n",
    "    population = scheduler.step(population, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5147fc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
